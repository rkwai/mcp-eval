# Copy to .env and adjust to run against real APIs or invoke live LLM evals.

# REST API target when you swap the mock adapter for a live integration.
# API_BASE_URL=https://api.internal-support.local
# API_DEFAULT_HEADERS={"X-Api-Key":"your-key"}
# API_BEARER_TOKEN=your-token
# API_TIMEOUT_MS=15000

# LLM evaluation defaults (OpenRouter compatible)
LLM_MODEL=openrouter/auto  # update to match the model you call
LLM_TEMPERATURE=0.1
LLM_MAX_TURNS=8

# Provider credentials (set the ones you intend to use)
LLM_PROVIDER_API_KEY=sk-your-api-key
LLM_PROVIDER_BASE_URL=https://openrouter.ai/api/v1/chat/completions

# Streamable HTTP transport (defaults shown)
# MCP_HTTP_HOST=0.0.0.0
# MCP_HTTP_PORT=3030
# MCP_HTTP_ALLOWED_HOSTS=localhost:3030
# MCP_HTTP_ALLOWED_ORIGINS=http://localhost:3030
# MCP_HTTP_ENABLE_DNS_PROTECTION=false

# Eval logging
EVAL_LOGS_ENABLED=false

# USEFUL HOSTED MODELS (FREE)
#LLM_MODEL=x-ai/grok-4-fast:free
#LLM_MODEL=google/gemini-2.0-flash-exp:free
#LLM_MODEL=deepseek/deepseek-chat-v3.1:free
#LLM_MODEL=qwen/qwen3-coder:free

# USE THESE SETTINGS FOR LOCALLY HOSTED MODEL (OLLAMA tooling)
#LLM_MODEL=hf.co/bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M
#LLM_PROVIDER_BASE_URL=http://localhost:11434/v1/chat/completions
