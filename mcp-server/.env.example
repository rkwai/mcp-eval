# Copy to .env and adjust to run against real APIs or invoke live LLM evals.

# REST API target when you swap the mock adapter for a live integration.
# API_BASE_URL=https://api.internal-support.local
# API_DEFAULT_HEADERS={"X-Api-Key":"your-key"}
# API_BEARER_TOKEN=your-token
# API_TIMEOUT_MS=15000

# LLM evaluation defaults
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
LLM_TEMPERATURE=0.1
LLM_MAX_TURNS=8

# Provider credentials (set the ones you intend to use)
# Leave API key blank for Ollama; provide it for hosted providers.
#LLM_PROVIDER_API_KEY=sk-your-api-key
LLM_PROVIDER_BASE_URL=http://localhost:11434

# Streamable HTTP transport (defaults shown)
# MCP_HTTP_HOST=0.0.0.0
# MCP_HTTP_PORT=3030
# MCP_HTTP_ALLOWED_HOSTS=localhost:3030
# MCP_HTTP_ALLOWED_ORIGINS=http://localhost:3030
# MCP_HTTP_ENABLE_DNS_PROTECTION=false

# Ax DSPy configuration
# AX_LLM_MODEL=openrouter/auto  # fallback to LLM_MODEL when unset
# AX_TEACHER_MODEL=openrouter/auto  # fallback to AX_LLM_MODEL when unset
AX_GEPA_ENABLED=true
AX_GEPA_OPTIMIZER=gepa
AX_GEPA_AUTO=light

# Eval logging
EVAL_LOGS_ENABLED=false

# USEFUL HOSTED MODELS (FREE)
#LLM_MODEL=x-ai/grok-4-fast:free
#LLM_MODEL=google/gemini-2.0-flash-exp:free
#LLM_MODEL=deepseek/deepseek-chat-v3.1:free
#LLM_MODEL=qwen/qwen3-coder:free

# USE THESE SETTINGS FOR LOCALLY HOSTED MODEL (OLLAMA tooling)
#LLM_MODEL=hf.co/bartowski/Qwen2.5-7B-Instruct-GGUF:Q4_K_M
#LLM_PROVIDER_BASE_URL=http://localhost:11434
